#!/usr/bin/env python3
"""
Analyse critique du syst√®me IA ChatNow
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from app.services.chatnow_service import OptimizedChatNowService
from app.core.config import settings
import logging
import time

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def critique_ia_system():
    """Analyse critique du syst√®me IA"""
    try:
        # Cr√©er l'engine de base de donn√©es
        engine = create_engine(settings.DATABASE_URL)
        SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
        db = SessionLocal()
        
        # Initialiser le service ChatNow
        chatnow_service = OptimizedChatNowService(db)
        
        logger.info("üîç ANALYSE CRITIQUE DU SYST√àME IA")
        logger.info("=" * 60)
        
        # Tests de diff√©rents types de questions
        test_cases = [
            {
                "category": "Question g√©n√©rale vague",
                "questions": [
                    "que dit la constitution sur les enfants",
                    "parle-moi de la constitution",
                    "quels sont les droits",
                    "comment √ßa marche"
                ]
            },
            {
                "category": "Question sp√©cifique avec num√©ro d'article",
                "questions": [
                    "que dit l'article 22",
                    "article 26",
                    "l'article 44 parle de quoi"
                ]
            },
            {
                "category": "Question avec mots-cl√©s pr√©cis",
                "questions": [
                    "dur√©e mandat pr√©sidentiel",
                    "√©lection pr√©sident",
                    "droits enfants jeunes"
                ]
            },
            {
                "category": "Question contextuelle",
                "questions": [
                    "√† quel √¢ge les enfants doivent aller √† l'√©cole",
                    "qui peut √™tre pr√©sident",
                    "comment sont prot√©g√©s les jeunes"
                ]
            }
        ]
        
        results = {}
        
        for test_case in test_cases:
            category = test_case["category"]
            questions = test_case["questions"]
            
            logger.info(f"\nüìã CAT√âGORIE: {category}")
            logger.info("-" * 40)
            
            category_results = []
            
            for question in questions:
                logger.info(f"\n‚ùì Question: {question}")
                
                start_time = time.time()
                
                # G√©n√©rer la r√©ponse
                response = chatnow_service.create_chat_response(
                    question=question,
                    chat_history=None,
                    user_id=f"critique_{category}_{questions.index(question)}"
                )
                
                end_time = time.time()
                response_time = end_time - start_time
                
                # Analyser la r√©ponse
                analysis = analyze_response_quality(question, response, response_time)
                category_results.append(analysis)
                
                logger.info(f"‚è±Ô∏è Temps de r√©ponse: {response_time:.2f}s")
                logger.info(f"üìù R√©ponse: {response[:100]}...")
                logger.info(f"üéØ Score: {analysis['score']}/10")
                logger.info(f"‚ö†Ô∏è Probl√®mes: {', '.join(analysis['issues'])}")
            
            results[category] = category_results
        
        # Analyse globale
        logger.info("\n" + "=" * 60)
        logger.info("üìä ANALYSE GLOBALE")
        logger.info("=" * 60)
        
        all_scores = []
        all_issues = []
        
        for category, category_results in results.items():
            category_scores = [r['score'] for r in category_results]
            avg_score = sum(category_scores) / len(category_scores)
            all_scores.extend(category_scores)
            
            logger.info(f"\nüìà {category}:")
            logger.info(f"   Score moyen: {avg_score:.2f}/10")
            logger.info(f"   Meilleur score: {max(category_scores)}/10")
            logger.info(f"   Pire score: {min(category_scores)}/10")
            
            for result in category_results:
                all_issues.extend(result['issues'])
        
        # Statistiques globales
        global_avg = sum(all_scores) / len(all_scores)
        logger.info(f"\nüåç SCORE GLOBAL MOYEN: {global_avg:.2f}/10")
        
        # Probl√®mes les plus fr√©quents
        issue_counts = {}
        for issue in all_issues:
            issue_counts[issue] = issue_counts.get(issue, 0) + 1
        
        logger.info(f"\nüö® PROBL√àMES IDENTIFI√âS:")
        for issue, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"   {issue}: {count} fois")
        
        # Recommandations
        logger.info(f"\nüí° RECOMMANDATIONS D'AM√âLIORATION:")
        recommendations = generate_recommendations(results, issue_counts)
        for i, rec in enumerate(recommendations, 1):
            logger.info(f"   {i}. {rec}")
        
        db.close()
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'analyse: {e}")
        return False

def analyze_response_quality(question: str, response: str, response_time: float) -> dict:
    """Analyse la qualit√© d'une r√©ponse"""
    score = 10.0
    issues = []
    
    # Crit√®res d'√©valuation
    
    # 1. Pertinence du contenu
    if "ne contient pas" in response.lower() or "n'est pas abord√©" in response.lower():
        score -= 4
        issues.append("Contenu non pertinent")
    
    # 2. Pr√©cision des informations
    if "article" in question.lower() and "article" not in response.lower():
        score -= 2
        issues.append("Num√©ro d'article manquant")
    
    # 3. Longueur de la r√©ponse
    if len(response) < 50:
        score -= 2
        issues.append("R√©ponse trop courte")
    elif len(response) > 500:
        score -= 1
        issues.append("R√©ponse trop longue")
    
    # 4. Temps de r√©ponse
    if response_time > 5.0:
        score -= 1
        issues.append("R√©ponse lente")
    
    # 5. Coh√©rence
    if "d√©sol√©" in response.lower() or "impossible" in response.lower():
        score -= 3
        issues.append("R√©ponse d'√©chec")
    
    # 6. Sp√©cificit√©
    if "g√©n√©ral" in response.lower() and "sp√©cifique" in question.lower():
        score -= 2
        issues.append("Manque de sp√©cificit√©")
    
    return {
        "score": max(0, score),
        "issues": issues,
        "response_time": response_time,
        "response_length": len(response)
    }

def generate_recommendations(results: dict, issue_counts: dict) -> list:
    """G√©n√®re des recommandations d'am√©lioration"""
    recommendations = []
    
    # Recommandations bas√©es sur les probl√®mes identifi√©s
    if "Contenu non pertinent" in issue_counts:
        recommendations.append("Am√©liorer l'algorithme de recherche s√©mantique")
    
    if "Num√©ro d'article manquant" in issue_counts:
        recommendations.append("Renforcer la d√©tection des r√©f√©rences d'articles")
    
    if "R√©ponse d'√©chec" in issue_counts:
        recommendations.append("Impl√©menter un syst√®me de fallback pour les questions non trouv√©es")
    
    if "Manque de sp√©cificit√©" in issue_counts:
        recommendations.append("Am√©liorer la g√©n√©ration de r√©ponses contextuelles")
    
    # Recommandations g√©n√©rales
    recommendations.extend([
        "Ajouter plus de donn√©es d'entra√Ænement pour am√©liorer la compr√©hension",
        "Impl√©menter un syst√®me de feedback utilisateur",
        "Optimiser les prompts pour l'API OpenAI",
        "Ajouter un syst√®me de cache plus intelligent",
        "Cr√©er des templates de r√©ponses pour les questions fr√©quentes"
    ])
    
    return recommendations

if __name__ == "__main__":
    success = critique_ia_system()
    if success:
        logger.info("\nüéâ Analyse critique termin√©e!")
    else:
        logger.error("‚ùå Analyse critique √©chou√©e!")
        sys.exit(1)
